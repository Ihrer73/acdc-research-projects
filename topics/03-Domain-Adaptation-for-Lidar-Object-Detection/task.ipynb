{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Task* - Domain Adaptation for Lidar Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Develop a methodology which optimizes a neural network for lidar object detection trained on public datasets with regard to its predictive performance on data affected by domain shift.\n",
    "\n",
    "- [Background and Motivation](#background-and-motivation)\n",
    "- [Task](#task)\n",
    "- [Required Tools and Data](#required-tools-and-data)\n",
    "- [Hints](#Hints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation\n",
    "\n",
    "The main purpose of supervised learning is to repeatedly serve a training algorithm with input-label-pairs (samples), s.t. the optimized model not only performs well on the training data, but also generalizes well to unseen data (validation/test data). Generalizing to unseen data that is in principle similar to training data seems easier than generalizing to unseen and very dissimilar data. One example is that a model, which is trained on image data from Aachen on a sunny day, will most likely perform better on other Aachen images of a sunny day than on New York images on a rainy day. Ideally, one would always be able to train models on training data similar to the target domain of where the model is supposed be used.\n",
    "\n",
    "The collection and labeling of suitable datasets for a particular supervised learning task, however, is usually associated with a lot of (manual) effort. In the case of point cloud segmentation, creating a new dataset requires the oftentimes manual annotation of every single point cloud in the dataset by associating every point cloud point with a particular semantic class. One example of a labeled point cloud from the [Waymo Open Dataset](https://waymo.com/open/data/perception/) is shown below.\n",
    "\n",
    "![](./assets/waymo-sample.jpg)\n",
    "\n",
    "Considering the labeling effort, it is desirable to make the most use out of publicly available datasets. Careful application of data augmentation, domain adaptation, and hyperparameter tuning techniques has the potential to improve generalization capabilities of trained models, allowing them to perform better on new domains (e.g., different sensor setup, different environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The task is to develop a methodology which optimizes a neural network for lidar object detection trained on public datasets with regard to its predictive performance on data affected by domain shift.\n",
    "\n",
    "### Subtasks\n",
    "\n",
    "> ***Note:*** *The subtasks listed below do not have to be followed strictly. They serve the purpose of guiding you along your own research for this topic.*\n",
    "\n",
    "1. Search for and choose a publicly available dataset for semantic point cloud segmentation related to driving (e.g. *Waymo Open Dataset*, *Argoverse2*).\n",
    "1. Research augmentation and domain adaptation techniques for semantic point cloud segmentation, such as ...\n",
    "   - rotation\n",
    "   - merging of classes (e.g., if training dataset distinguishes between cars and buses, but target domain only cares about vehicles in general)\n",
    "   - use of some (labeled) target domain samples in training dataset, possibly given extra weight as compared to standard samples\n",
    "   - ...\n",
    "1. Research training techniques to improve generalization, such as ...\n",
    "   - dropout\n",
    "   - L1/L2 regularization\n",
    "   - ...\n",
    "1. Implement a TensorFlow data pipeline, possibly including online data augmentation.\n",
    "1. Implement a TensorFlow model for lidar object detection.\n",
    "1. Train a model on the selected public dataset and evaluate its performance on the dataset's validation set and ika's validation dataset (suggested metric: *Validation Loss*).\n",
    "1. Iterate on the trainings with different augmentation, domain adaptation, generalization techniques, and other hyperparameters in order to optimize generalization capabilities of the trained models, especially with regard to ika's validation dataset.\n",
    "1. Document your research, developed approach, and evaluations in a Jupyter notebook report. Explain and reproduce individual parts of your implemented functions with exemplary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Tools and Data\n",
    "\n",
    "### Tools\n",
    "\n",
    "- TensorFlow\n",
    "\n",
    "### Data\n",
    "\n",
    "- ika's validation dataset\n",
    "- *(to be found)* publicly available dataset for semantic point cloud segmentation related to driving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "### Relevant ACDC Sections\n",
    "\n",
    "- **Sensor Data Processing Algorithms**\n",
    "  - Object Detection\n",
    "\n",
    "### TFDS Datasets\n",
    "\n",
    "[TensorFlow Datasets](https://www.tensorflow.org/datasets) is a collection of datasets ready-to-use with TensorFlow. It may already contain datasets interesting for semantic point cloud segmentation and thus save you from worrying about parsing data files from disk. All TFDS datasets are exposed as [`tf.data.Dataets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), ready to be passed to [`model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).\n",
    "\n",
    "The example below shows how easy it is to load a TFDS dataset, in this case the famous [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist). Note that this dataset is not related to semantic point cloud segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.11.0\n",
      "  latest version: 4.12.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: pandas in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (1.4.1)\n",
      "Requirement already satisfied: tensorflow-datasets==4.4.0 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (4.4.0)\n",
      "Requirement already satisfied: dill in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (0.3.4)\n",
      "Requirement already satisfied: numpy in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (1.22.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (1.7.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (2.27.1)\n",
      "Requirement already satisfied: future in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (0.18.2)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (3.19.4)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (21.4.0)\n",
      "Requirement already satisfied: termcolor in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (1.1.0)\n",
      "Requirement already satisfied: six in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (1.15.0)\n",
      "Requirement already satisfied: promise in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (2.3)\n",
      "Requirement already satisfied: absl-py in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (4.63.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (2021.10.8)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/reiher/.miniconda3/envs/acdc-rp/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow-datasets==4.4.0) (1.56.0)\n"
     ]
    }
   ],
   "source": [
    "# install required Python packages\n",
    "\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} -c conda-forge \\\n",
    "    tensorflow=2.6.0\n",
    "!{sys.executable} -m pip install \\\n",
    "    pandas \\\n",
    "    tensorflow-datasets==4.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFDS MNIST dataset contains three different splits of data:\n",
      "{'test': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>, 'train': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}\n",
      "The dataset has two features, 'image' and 'label'. Here is one sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 15:17:18.137511: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_56ed3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_56ed3_level0_col0\" class=\"col_heading level0 col0\" >image</th>\n",
       "      <th id=\"T_56ed3_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_56ed3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_56ed3_row0_col0\" class=\"data row0 col0\" >[[[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 84]\n",
       "  [254]\n",
       "  [101]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [174]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 31]\n",
       "  [247]\n",
       "  [202]\n",
       "  [ 29]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  1]\n",
       "  [  1]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [141]\n",
       "  [253]\n",
       "  [168]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 66]\n",
       "  [208]\n",
       "  [ 56]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [186]\n",
       "  [253]\n",
       "  [120]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 57]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 28]\n",
       "  [249]\n",
       "  [240]\n",
       "  [ 25]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 34]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [109]\n",
       "  [254]\n",
       "  [197]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 53]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [135]\n",
       "  [254]\n",
       "  [133]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [133]\n",
       "  [254]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 27]\n",
       "  [240]\n",
       "  [255]\n",
       "  [ 35]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  7]\n",
       "  [235]\n",
       "  [253]\n",
       "  [208]\n",
       "  [151]\n",
       "  [169]\n",
       "  [215]\n",
       "  [253]\n",
       "  [206]\n",
       "  [  2]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 97]\n",
       "  [253]\n",
       "  [253]\n",
       "  [253]\n",
       "  [254]\n",
       "  [253]\n",
       "  [253]\n",
       "  [253]\n",
       "  [ 86]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [150]\n",
       "  [244]\n",
       "  [145]\n",
       "  [119]\n",
       "  [101]\n",
       "  [ 82]\n",
       "  [253]\n",
       "  [253]\n",
       "  [ 14]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 84]\n",
       "  [254]\n",
       "  [172]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [174]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [237]\n",
       "  [252]\n",
       "  [ 56]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 50]\n",
       "  [241]\n",
       "  [182]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [187]\n",
       "  [254]\n",
       "  [249]\n",
       "  [105]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [186]\n",
       "  [253]\n",
       "  [206]\n",
       "  [ 21]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [227]\n",
       "  [242]\n",
       "  [ 32]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [236]\n",
       "  [219]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]]</td>\n",
       "      <td id=\"T_56ed3_row0_col1\" class=\"data row0 col1\" >4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load MNIST\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds = tfds.load(\"mnist\")\n",
    "print(\"The TFDS MNIST dataset contains three different splits of data:\")\n",
    "print(ds)\n",
    "print(\"The dataset has two features, 'image' and 'label'. Here is one sample:\")\n",
    "tfds.as_dataframe(ds[\"train\"].take(1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f35c820e40155ce3a3d6b6baab4aa8cb626eff9596fe63e71a966e5e0dc1513e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('acdc-rp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
